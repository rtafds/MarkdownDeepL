# ワークロードを考慮した匿名化

Kristen LeFevre $$^{1} \quad$  David J. DeWitt $${ }^{1} \quad$  Raghu Ramakrishnan $${ }^{1,2}$ <br>  $${ }^{1}$  University of Wisconsin - Madison, 1210 West Dayton St., Madison, WI 53706<br> $${ }^{2}$  Yahoo! Research, 701 First Ave., Sunnyvale, CA 94089


#### 要旨

データのプライバシー保護は、マイクロデータの配布において重要な問題である。匿名化アルゴリズムは通常、個人のプライバシーを保護し、結果として得られるデータの品質への影響を最小限に抑えることを目的としている。これまでの研究の大部分は、一律に品質を測定してきたが、我々は、品質とは、データが最終的に使用される作業負荷に関して最もよく判断されるものであると主張する。

本論文では、1つまたは複数のデータマイニングタスクと選択述語からなる作業負荷のターゲットクラスに基づいて匿名化ビューを生成する一連の匿名化アルゴリズムを提供します。広範な実験的評価により、このアプローチが、これまでの匿名化技術よりもしばしば効果的であることが示された。


### 単一ターゲット分類モデル

多次元再符号化を用いた匿名化のために、Mondrianアルゴリズムが最近提案された[17]。このアルゴリズムは、（多次元）準識別子領域空間の貪欲な再帰的分割に基づく（図3参照）。ほぼ均一なパーティション占有率を得るために、[17]は、正規化された値の範囲が最大の分割属性を再帰的に選択し、（連続または順序属性の場合）分割属性の中央値を中心にデータをパーティション化することを提案している。このプロセスは、許容される分割がなくなるまで繰り返される。つまり、匿名性の制約や値の汎化階層によって課される制約に違反しない限り、特定の領域をさらに分割することができないことを意味している。このアルゴリズムをMedian Mondrianと呼ぶ。

ターゲットマイニングモデル（のセット）がわかっている場合、このヒューリスティックを改良することができる。まず、予測属性 $$Q_{1}, \ldots, Q_{d}$  （準識別子でもある）とクラスラベル $$C$  を持つ、単一のターゲット分類モデルを考える。この場合、情報利得に基づく発見的な分割スキームを提案するが、これは決定木の構築を連想させる。直感的には、この貪欲な基準の目標は、クラスラベルの均質なパーティションを生成することである。

各再帰ステップで、（匿名性制約に違反することなく）結果のパーティションセットに対する重み付けエントロピーを最小化する分割を選択する。 $$P$  は現在の（再帰的）タプルセットを示し、パーティション $$P^{\prime}$  は候補分割から得られるパーティションのセットを示す。 $$p\left(c \mid P^{\prime}\right)$  は $$P^{\prime}$  のタプルのうちクラスラベル $$C=c$  を持つ割合である。このアルゴリズムをInfoGain Mondrianと呼ぶことにする。

\begin{align*}
\operatorname{Entropy}(P, C)=\sum_{\text {partitions } P^{\prime}} \frac{\left|P^{\prime}\right|}{|P|} \sum_{c \in D_{C}}-p\left(c \mid P^{\prime}\right) \log p\left(c \mid P^{\prime}\right)
\end{align*}

InfoGain Mondrianは、決定木で一般的に扱われるように、連続した準識別値を扱い、最小のエントロピーを持つ閾値を中心にパーティショニングを行います（[12]を参照）。データはまず、分割された属性に関してソートされる。その後、データをスキャンし、クラスラベルに変化があるたびに、匿名性とエントロピーに関してこの閾値候補をチェックする。匿名性の制約を満たす閾値候補がない場合、デフォルトとして中央値もチェックされる。

InfoGain Mondrianは、RainForest [14]のような既存のスケーラブルな決定木誘導スキームをそのまま適応することで、大規模なデータセットにスケールします。
